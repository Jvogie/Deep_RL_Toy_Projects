{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jvogie/Deep_RL_Toy_Projects/blob/main/PPO_Lunar_Lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDAH0h0EBiG"
      },
      "source": [
        "## Install dependencies and create a virtual screen 🔽\n",
        "\n",
        "The first step is to install the dependencies, we’ll install multiple ones.\n",
        "\n",
        "- `gymnasium[box2d]`: Contains the LunarLander-v2 environment 🌛\n",
        "- `stable-baselines3[extra]`: The deep reinforcement learning library.\n",
        "- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face 🤗 Hub.\n",
        "\n",
        "To make things easier, we created a script to install all these dependencies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install swig cmake"
      ],
      "metadata": {
        "id": "yQIGLPDkGhgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install virtual screen libraries and create and run a virtual screen 🖥"
      ],
      "metadata": {
        "id": "BEKeXQJsQCYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "j5f2cGkdP-mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**"
      ],
      "metadata": {
        "id": "TCwBTAwAW9JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "cYvkbef7XEMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "BE5JWP5rQIKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cygWLPGsEQ0m"
      },
      "outputs": [],
      "source": [
        "import gymnasium\n",
        "\n",
        "from huggingface_sb3 import load_from_hub, package_to_hub\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7vOFlpA_ONz"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "# First, we create our environment called LunarLander-v2\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# Then we reset this environment\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(20):\n",
        "  # Take a random action\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "\n",
        "  # Do this action in the environment and get\n",
        "  # next_state, reward, terminated, truncated and info\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
        "  if terminated or truncated:\n",
        "      # Reset the environment\n",
        "      print(\"Environment is reset\")\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poLBgRocF9aT"
      },
      "source": [
        "Let's see what the Environment looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [],
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, where each value contains different information about the lander:\n",
        "- Horizontal pad coordinate (x)\n",
        "- Vertical pad coordinate (y)\n",
        "- Horizontal speed (x)\n",
        "- Vertical speed (y)\n",
        "- Angle\n",
        "- Angular speed\n",
        "- If the left leg contact point has touched the land (boolean)\n",
        "- If the right leg contact point has touched the land (boolean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available 🎮:\n",
        "\n",
        "- Action 0: Do nothing,\n",
        "- Action 1: Fire left orientation engine,\n",
        "- Action 2: Fire the main engine,\n",
        "- Action 3: Fire right orientation engine.\n",
        "\n",
        "Reward function (the function that will gives a reward at each timestep) 💰:\n",
        "\n",
        "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
        "\n",
        "For each step, the reward:\n",
        "\n",
        "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
        "-  Is increased/decreased the slower/faster the lander is moving.\n",
        "- Is decreased the more the lander is tilted (angle not horizontal).\n",
        "- Is increased by 10 points for each leg that is in contact with the ground.\n",
        "- Is decreased by 0.03 points each frame a side engine is firing.\n",
        "- Is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
        "\n",
        "An episode is **considered a solution if it scores at least 200 points.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFD9RAFjG8aq"
      },
      "source": [
        "#### Vectorized Environment\n",
        "\n",
        "- We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99hqQ_etEy1N"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "env = make_vec_env('LunarLander-v2', n_envs=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxI6hT1GE4-A"
      },
      "outputs": [],
      "source": [
        "# TODO: Define a PPO MlpPolicy architecture\n",
        "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
        "# if we had frames as input we would use CnnPolicy\n",
        "model =PPO(\n",
        "    policy = 'MlpPolicy',\n",
        "    env = env,\n",
        "    n_steps = 1024,\n",
        "    batch_size = 64,\n",
        "    n_epochs = 4,\n",
        "    gamma = 0.999,\n",
        "    gae_lambda = 0.98,\n",
        "    ent_coef = 0.01,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJJk88yoBUi"
      },
      "source": [
        "## Train the PPO agent 🏃\n",
        "- Let's train our agent for 1,000,000 timesteps, don't forget to use GPU on Colab. It will take approximately ~20min, but you can use fewer timesteps if you just want to try it out.\n",
        "- During the training, take a ☕ break you deserved it 🤗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poBCy9u_csyR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train it for 1,000,000 timesteps\n",
        "model.learn(total_timesteps=1000000)\n",
        "# Save the model\n",
        "model_name = \"ppo-LunarLander-v2\"\n",
        "model.save(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY_HuedOoISR"
      },
      "source": [
        "## Evaluate the agent 📈\n",
        "- Remember to wrap the environment in a [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html).\n",
        "- Now that our Lunar Lander agent is trained 🚀, we need to **check its performance**.\n",
        "- Stable-Baselines3 provides a method to do that: `evaluate_policy`.\n",
        "- To fill that part you need to [check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\n",
        "- In the next step,  we'll see **how to automatically evaluate and share your agent to compete in a leaderboard, but for now let's do it ourselves**\n",
        "\n",
        "\n",
        "💡 When you evaluate your agent, you should not use your training environment but create an evaluation environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpz8kHlt_a_m"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the model I download from the Hub was trained with Gym (the former version of Gymnasium) we need to install shimmy a API conversion tool that will help us to run the environment correctly.\n",
        "\n",
        "Shimmy Documentation: https://github.com/Farama-Foundation/Shimmy"
      ],
      "metadata": {
        "id": "bhb9-NtsinKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shimmy"
      ],
      "metadata": {
        "id": "03WI-bkci1kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj8PSGHJfwz3"
      },
      "outputs": [],
      "source": [
        "from huggingface_sb3 import load_from_hub\n",
        "repo_id = \"Classroom-workshop/assignment2-omar\" # The repo_id\n",
        "filename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n",
        "\n",
        "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
        "# But Python 3.6, 3.7 use protocol 4\n",
        "# In order to get compatibility we need to:\n",
        "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
        "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
        "custom_objects = {\n",
        "            \"learning_rate\": 0.0,\n",
        "            \"lr_schedule\": lambda _: 0.0,\n",
        "            \"clip_range\": lambda _: 0.0,\n",
        "}\n",
        "\n",
        "checkpoint = load_from_hub(repo_id, filename)\n",
        "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs0Y-qgPgLUf"
      },
      "source": [
        "Let's evaluate this agent:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "id": "PAEVwK-aahfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "QAN7B0_HCVZC",
        "BqPKw3jt_pG5"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}